{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398d3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d76b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "cols_to_drop = [\"id\", \"date\", \"zipcode\"]\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# scale price\n",
    "train_df[\"price\"] = train_df[\"price\"] / 1000\n",
    "test_df[\"price\"] = test_df[\"price\"] / 1000\n",
    "\n",
    "X_train = train_df.drop(columns=[\"price\"])\n",
    "y_train = train_df[\"price\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"price\"])\n",
    "y_test = test_df[\"price\"]\n",
    "\n",
    "# scale each feature so that the mean is 0, and the standard deviation is 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af8dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Coefficients:\n",
      "          Feature  Coefficient\n",
      "0      Unnamed: 0     8.456024\n",
      "1        bedrooms   -12.807339\n",
      "2       bathrooms    18.456913\n",
      "3     sqft_living    57.161582\n",
      "4        sqft_lot    11.127338\n",
      "5          floors     8.151038\n",
      "6      waterfront    64.230911\n",
      "7            view    47.610288\n",
      "8       condition    12.647609\n",
      "9           grade    92.511076\n",
      "10     sqft_above    48.439051\n",
      "11  sqft_basement    27.688812\n",
      "12       yr_built   -68.043173\n",
      "13   yr_renovated    17.341926\n",
      "14            lat    78.129852\n",
      "15           long    -1.437669\n",
      "16  sqft_living15    45.479128\n",
      "17     sqft_lot15   -12.906560\n"
     ]
    }
   ],
   "source": [
    "# training mlr\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "coef_table = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "})\n",
    "\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "print(coef_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab96184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MSE: 31415.747916100867\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"\\nTraining MSE:\", train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1c6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2: 0.7271450489303788\n"
     ]
    }
   ],
   "source": [
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "print(\"Training R^2:\", train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "622e0626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing MSE: 58834.673978213985\n",
      "Testing R^2: 0.6471195893437872\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTesting MSE:\", test_mse)\n",
    "print(\"Testing R^2:\", test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c437fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column of ones to design matrix \n",
    "X_train_np = X_train_scaled\n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "X_train_aug = np.hstack([np.ones((X_train_np.shape[0], 1)), X_train_np])\n",
    "\n",
    "# closed form solution\n",
    "theta = np.linalg.inv(X_train_aug.T @ X_train_aug) @ X_train_aug.T @ y_train_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadcf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_new_scaled, theta):\n",
    "    \"\"\"\n",
    "    X_new_scaled : numpy array of shape (n_samples, n_features)\n",
    "    theta        : numpy array of shape (n_features+1, 1)\n",
    "    \"\"\"\n",
    "    # augment with col of 1s\n",
    "    X_new_aug = np.hstack([np.ones((X_new_scaled.shape[0], 1)), X_new_scaled])\n",
    "    return X_new_aug @ theta \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bf6713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_manual = predict(X_train_scaled, theta)\n",
    "y_test_pred_manual = predict(X_test_scaled, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fb6503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Linear Regression - Training MSE: 36000.67530645922\n",
      "Manual Linear Regression - Training R^2: 0.6873236147218099\n",
      "Manual Linear Regression - Testing MSE: 62811.14764798538\n",
      "Manual Linear Regression - Testing R^2: 0.6232693737027156\n"
     ]
    }
   ],
   "source": [
    "# compute metrics\n",
    "train_mse_manual = mean_squared_error(y_train, y_train_pred_manual)\n",
    "train_r2_manual = r2_score(y_train, y_train_pred_manual)\n",
    "\n",
    "test_mse_manual = mean_squared_error(y_test, y_test_pred_manual)\n",
    "test_r2_manual = r2_score(y_test, y_test_pred_manual)\n",
    "\n",
    "print(\"Manual Linear Regression - Training MSE:\", train_mse_manual)\n",
    "print(\"Manual Linear Regression - Training R^2:\", train_r2_manual)\n",
    "print(\"Manual Linear Regression - Testing MSE:\", test_mse_manual)\n",
    "print(\"Manual Linear Regression - Testing R^2:\", test_r2_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bcea3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, degree):\n",
    "    X = np.asarray(X).reshape(-1, 1)\n",
    "    return np.hstack([X**d for d in range(1, degree + 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b5093b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial_regression(X, y, degree):\n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    X_augmented = np.hstack([np.ones((X_poly.shape[0], 1)), X_poly])\n",
    "    y = y.reshape(-1, 1)\n",
    "    theta = np.linalg.inv(X_augmented.T @ X_augmented) @ X_augmented.T @ y\n",
    "    return theta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b107d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_polynomial(X, theta, degree):\n",
    "    \n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    X_aug = np.hstack([np.ones((X_poly.shape[0], 1)), X_poly])\n",
    "    \n",
    "    return X_aug @ theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83128f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_closed_form(X, y):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "    return theta\n",
    "def predict_closed_form(X, theta):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return X_b @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b7b439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sqft = X_train[['sqft_living']].values\n",
    "X_test_sqft  = X_test[['sqft_living']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697b210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train poly shape: (1000, 1)\n",
      "Train poly shape: (1000, 2)\n",
      "Train poly shape: (1000, 3)\n",
      "Train poly shape: (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "degrees = [1, 2, 3, 5]\n",
    "\n",
    "for p in degrees:\n",
    "    \n",
    "    X_train_poly = polynomial_features(X_train_sqft, p)\n",
    "    X_test_poly  = polynomial_features(X_test_sqft, p)\n",
    "    \n",
    "    theta = fit_closed_form(X_train_poly, y_train)\n",
    "    \n",
    "    y_train_pred = predict_closed_form(X_train_poly, theta)\n",
    "    y_test_pred  = predict_closed_form(X_test_poly, theta)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse  = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2  = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    results.append([p, train_mse, test_mse, train_r2, test_r2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1af6d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Degree p     Train MSE      Test MSE  Train R²     Test R²\n",
      "0         1  57947.526161  8.857598e+04  0.496709    0.468736\n",
      "1         2  54822.665116  7.179168e+04  0.523849    0.569406\n",
      "2         3  53785.194716  9.983348e+04  0.532860    0.401216\n",
      "3         5  52626.111955  2.865728e+07  0.542927 -170.881541\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Degree p\", \"Train MSE\", \"Test MSE\", \"Train R²\", \"Test R²\"]\n",
    ")\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387d067",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44b5540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = X @ theta\n",
    "    error = predictions - y\n",
    "    return (1 / (2 * m)) * np.sum(error**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86980126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha = 0.01, num_iters = 1000):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for _ in range(num_iters):\n",
    "        predictions = X @ theta\n",
    "        gradient = (1/m) * (X.T @ (predictions - y))\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "        cost_history.append(compute_cost(X, y, theta))\n",
    "    return theta, cost_history\n",
    "\n",
    "def add_intercept(X):\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a74fc966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameters: [520.414834     8.45596481 -12.80702978  18.45703883  57.16053175\n",
      "  11.12163759   8.15140424  64.23053237  47.60980377  12.64760666\n",
      "  92.50946386  48.43786851  27.6888169  -68.04271631  17.34219195\n",
      "  78.13001983  -1.43845917  45.48258088 -12.90078962]\n"
     ]
    }
   ],
   "source": [
    "X_train_gd = add_intercept(X_train_scaled)\n",
    "\n",
    "theta_init = np.zeros(X_train_gd.shape[1])\n",
    "\n",
    "theta_gd, cost_history = gradient_descent(\n",
    "    X_train_gd,\n",
    "    y_train,\n",
    "    theta_init,\n",
    "    alpha=0.01,\n",
    "    num_iters=5000\n",
    ")\n",
    "\n",
    "print(\"Learned parameters:\", theta_gd)\n",
    "\n",
    "def predict_gd(X, theta):\n",
    "    X = add_intercept(X)\n",
    "    return X @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ee65e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alpha  iterations                   theta_norm  \\\n",
      "0 0.0100          10                      67.6553   \n",
      "1 0.0100          50                     238.4885   \n",
      "2 0.0100         100                     362.3810   \n",
      "3 0.1000          10                     371.7395   \n",
      "4 0.1000          50                     548.9296   \n",
      "5 0.1000         100                     552.7541   \n",
      "6 0.5000          10                   10793.9108   \n",
      "7 0.5000          50           1810779882738.0154   \n",
      "8 0.5000         100 34246110495022262517760.0000   \n",
      "\n",
      "                                           train_MSE  \\\n",
      "0                                        294796.6946   \n",
      "1                                        138298.5227   \n",
      "2                                         70094.3812   \n",
      "3                                         66473.6109   \n",
      "4                                         31510.7162   \n",
      "5                                         31427.5040   \n",
      "6                                     616361041.0041   \n",
      "7                    17084644118527006302273536.0000   \n",
      "8 6110786660150638353353561641250418165249212416....   \n",
      "\n",
      "                                            test_MSE  \\\n",
      "0                                        352448.9596   \n",
      "1                                        170450.9279   \n",
      "2                                         94751.2153   \n",
      "3                                         90873.0089   \n",
      "4                                         58929.6289   \n",
      "5                                         58891.7417   \n",
      "6                                     688809805.7339   \n",
      "7                    19044809143534992998006784.0000   \n",
      "8 6811892881724131399234250712862696346054819840....   \n",
      "\n",
      "                                         train_R2  \\\n",
      "0                                         -1.5604   \n",
      "1                                         -0.2012   \n",
      "2                                          0.3912   \n",
      "3                                          0.4227   \n",
      "4                                          0.7263   \n",
      "5                                          0.7270   \n",
      "6                                      -5352.2758   \n",
      "7                     -148385126703077752832.0000   \n",
      "8 -53073967858577056396204862496020545667072.0000   \n",
      "\n",
      "                                          test_R2  \n",
      "0                                         -1.1139  \n",
      "1                                         -0.0223  \n",
      "2                                          0.4317  \n",
      "3                                          0.4550  \n",
      "4                                          0.6466  \n",
      "5                                          0.6468  \n",
      "6                                      -4130.3646  \n",
      "7                     -114227539935521472512.0000  \n",
      "8 -40856579885851774077279061072198846382080.0000  \n"
     ]
    }
   ],
   "source": [
    "X_train_gd = add_intercept(X_train_scaled)\n",
    "X_test_gd  = add_intercept(X_test_scaled)\n",
    "\n",
    "theta_init = np.zeros(X_train_gd.shape[1])\n",
    "\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "iterations_list = [10, 50, 100]\n",
    "\n",
    "results = []\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    for iters in iterations_list:\n",
    "        \n",
    "        theta, _ = gradient_descent(\n",
    "            X_train_gd,\n",
    "            y_train,\n",
    "            theta_init.copy(),\n",
    "            alpha,\n",
    "            iters\n",
    "        )\n",
    "        y_train_pred = X_train_gd @ theta\n",
    "        y_test_pred  = X_test_gd @ theta\n",
    "    \n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse  = mean_squared_error(y_test, y_test_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2  = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"iterations\": iters,\n",
    "            \"theta_norm\": np.linalg.norm(theta),\n",
    "            \"train_MSE\": train_mse,\n",
    "            \"test_MSE\": test_mse,\n",
    "            \"train_R2\": train_r2,\n",
    "            \"test_R2\": test_r2\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_csv(\"gradient_descent_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0611e70",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e133ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_gradient_descent(X, y, alpha=0.01, iterations=100, lam=0.1):\n",
    "    N, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        \n",
    "        gradient = (2/N) * (X.T @ errors)\n",
    "        \n",
    "        reg = 2 * lam * theta\n",
    "        \n",
    "        reg[0] = 0\n",
    "        \n",
    "        theta = theta - alpha * (gradient + reg)\n",
    "    \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "754dea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lambda    alpha  iterations  \\\n",
      "0  0.010000 0.010000          10   \n",
      "1  0.010000 0.010000          50   \n",
      "2  0.010000 0.010000         100   \n",
      "3  0.010000 0.100000          10   \n",
      "4  0.010000 0.100000          50   \n",
      "5  0.010000 0.100000         100   \n",
      "6  0.010000 0.500000          10   \n",
      "7  0.010000 0.500000          50   \n",
      "8  0.010000 0.500000         100   \n",
      "9  0.100000 0.010000          10   \n",
      "10 0.100000 0.010000          50   \n",
      "11 0.100000 0.010000         100   \n",
      "12 0.100000 0.100000          10   \n",
      "13 0.100000 0.100000          50   \n",
      "14 0.100000 0.100000         100   \n",
      "15 0.100000 0.500000          10   \n",
      "16 0.100000 0.500000          50   \n",
      "17 0.100000 0.500000         100   \n",
      "18 1.000000 0.010000          10   \n",
      "19 1.000000 0.010000          50   \n",
      "20 1.000000 0.010000         100   \n",
      "21 1.000000 0.100000          10   \n",
      "22 1.000000 0.100000          50   \n",
      "23 1.000000 0.100000         100   \n",
      "24 1.000000 0.500000          10   \n",
      "25 1.000000 0.500000          50   \n",
      "26 1.000000 0.500000         100   \n",
      "\n",
      "                                           theta_norm  \\\n",
      "0                                          122.146407   \n",
      "1                                          363.210721   \n",
      "2                                          482.090057   \n",
      "3                                          495.097455   \n",
      "4                                          552.268169   \n",
      "5                                          552.783410   \n",
      "6                                    171343282.403565   \n",
      "7           1770872058785638794495668795736064.000000   \n",
      "8  32816103431931782124689297127498254440049049479...   \n",
      "9                                          121.792832   \n",
      "10                                         361.644453   \n",
      "11                                         479.881420   \n",
      "12                                         492.838912   \n",
      "13                                         548.500726   \n",
      "14                                         548.617321   \n",
      "15                                   208009138.939701   \n",
      "16          4999852593671062481868036620943360.000000   \n",
      "17 26610340754870321129047262756583412782913067371...   \n",
      "18                                         118.519257   \n",
      "19                                         350.570161   \n",
      "20                                         467.102969   \n",
      "21                                         479.893662   \n",
      "22                                         534.210045   \n",
      "23                                         534.217281   \n",
      "24                                  1184726907.835685   \n",
      "25      56053516987763654988074185539515842560.000000   \n",
      "26 39114123426752359731326245994329669029620736690...   \n",
      "\n",
      "                                            train_MSE  \\\n",
      "0                                       235760.658497   \n",
      "1                                        69735.301208   \n",
      "2                                        36789.980070   \n",
      "3                                        35068.398153   \n",
      "4                                        31434.435721   \n",
      "5                                        31420.267235   \n",
      "6                           152971056020584544.000000   \n",
      "7  16339884633290686398793383650165887703404055086...   \n",
      "8  56111081362452333063799966644529860553234121890...   \n",
      "9                                       236027.030973   \n",
      "10                                       70120.330159   \n",
      "11                                       37088.465494   \n",
      "12                                       35334.289004   \n",
      "13                                       31672.269431   \n",
      "14                                       31659.351506   \n",
      "15                          225444605040922144.000000   \n",
      "16 13025338422378718292770948088809659481638294554...   \n",
      "17 36895677240072443841825531587168004049101655995...   \n",
      "18                                      238685.339764   \n",
      "19                                       75662.886841   \n",
      "20                                       43362.327321   \n",
      "21                                       41640.210681   \n",
      "22                                       38464.838718   \n",
      "23                                       38464.838470   \n",
      "24                         7313262905541514240.000000   \n",
      "25 16371193756101724296134058636709959384183127137...   \n",
      "26 79715324517668644169422329343743969009923972738...   \n",
      "\n",
      "                                             test_MSE  \\\n",
      "0                                       282913.686528   \n",
      "1                                        94446.918425   \n",
      "2                                        61383.671214   \n",
      "3                                        60095.503542   \n",
      "4                                        58927.218450   \n",
      "5                                        58874.794721   \n",
      "6                           170521837297755360.000000   \n",
      "7  18214601492725409804660957528912005074543598035...   \n",
      "8  62548849595955350181002949849882777866353521318...   \n",
      "9                                       283334.984394   \n",
      "10                                       95602.010740   \n",
      "11                                       62383.578509   \n",
      "12                                       60993.684562   \n",
      "13                                       59464.270321   \n",
      "14                                       59433.691536   \n",
      "15                          251310458921130656.000000   \n",
      "16 14519768896540431097196053133049544982686075583...   \n",
      "17 41128812890326785750128150465480996659580912279...   \n",
      "18                                      287475.705474   \n",
      "19                                      107451.107796   \n",
      "20                                       74224.104744   \n",
      "21                                       72509.044633   \n",
      "22                                       69591.769134   \n",
      "23                                       69591.808679   \n",
      "24                         8152317124334226432.000000   \n",
      "25 18249502791473139430575185186789340292833844889...   \n",
      "26 88861268089639102434667618377556045649517137239...   \n",
      "\n",
      "                                             train_R2  \\\n",
      "0                                           -1.047650   \n",
      "1                                            0.394329   \n",
      "2                                            0.680468   \n",
      "3                                            0.695421   \n",
      "4                                            0.726983   \n",
      "5                                            0.727106   \n",
      "6                               -1328598323267.735352   \n",
      "7  -1419166729376130636943048862876223619591718962...   \n",
      "8  -4873411386067575073414900159761122993305241391...   \n",
      "9                                           -1.049964   \n",
      "10                                           0.390984   \n",
      "11                                           0.677876   \n",
      "12                                           0.693111   \n",
      "13                                           0.724917   \n",
      "14                                           0.725029   \n",
      "15                              -1958052274979.999268   \n",
      "16 -1131288705077089624532522701680844081672727313...   \n",
      "17 -3204497386121750044348286375724810397828133591...   \n",
      "18                                          -1.073052   \n",
      "19                                           0.342846   \n",
      "20                                           0.623386   \n",
      "21                                           0.638343   \n",
      "22                                           0.665922   \n",
      "23                                           0.665922   \n",
      "24                             -63517825441554.500000   \n",
      "25 -1421886018184868727084940987040126326559044547...   \n",
      "26 -6923508881232143261186290980794214182826525775...   \n",
      "\n",
      "                                              test_R2  \n",
      "0                                           -0.696868  \n",
      "1                                            0.433523  \n",
      "2                                            0.631831  \n",
      "3                                            0.639557  \n",
      "4                                            0.646565  \n",
      "5                                            0.646879  \n",
      "6                               -1022761101620.225830  \n",
      "7  -1092480950446379492810201815742016353696567851...  \n",
      "8  -3751574070023333045464660738721581605802174210...  \n",
      "9                                           -0.699395  \n",
      "10                                           0.426595  \n",
      "11                                           0.625834  \n",
      "12                                           0.634170  \n",
      "13                                           0.643343  \n",
      "14                                           0.643527  \n",
      "15                              -1507317572271.573730  \n",
      "16 -8708711486600186442081683995851440861426770553...  \n",
      "17 -2466836544027643752099302510865745417011658362...  \n",
      "18                                          -0.724231  \n",
      "19                                           0.355526  \n",
      "20                                           0.554816  \n",
      "21                                           0.565103  \n",
      "22                                           0.582600  \n",
      "23                                           0.582600  \n",
      "24                             -48896217487326.132812  \n",
      "25 -1094574271238654823654247289596768194620840598...  \n",
      "26 -5329748370240831459196282194857458477749592530...  \n"
     ]
    }
   ],
   "source": [
    "X_train_gd = add_intercept(X_train_scaled)\n",
    "X_test_gd  = add_intercept(X_test_scaled)\n",
    "\n",
    "theta_init = np.zeros(X_train_gd.shape[1])\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "iterations_list = [10, 50, 100]\n",
    "lambdas = [0.01, 0.1, 1.0]  \n",
    "\n",
    "results = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    for alpha in learning_rates:\n",
    "        for iters in iterations_list:\n",
    "            \n",
    "            theta = ridge_gradient_descent(\n",
    "                X_train_gd,\n",
    "                y_train,\n",
    "                alpha=alpha,\n",
    "                iterations=iters,\n",
    "                lam=lam\n",
    "            )\n",
    "            \n",
    "            y_train_pred = X_train_gd @ theta\n",
    "            y_test_pred  = X_test_gd @ theta\n",
    "        \n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            test_mse  = mean_squared_error(y_test, y_test_pred)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            test_r2  = r2_score(y_test, y_test_pred)\n",
    "            \n",
    "            results.append({\n",
    "                \"lambda\": lam,\n",
    "                \"alpha\": alpha,\n",
    "                \"iterations\": iters,\n",
    "                \"theta_norm\": np.linalg.norm(theta),\n",
    "                \"train_MSE\": train_mse,\n",
    "                \"test_MSE\": test_mse,\n",
    "                \"train_R2\": train_r2,\n",
    "                \"test_R2\": test_r2\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_csv(\"ridge_gradient_descent_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "409dbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N = 1000\n",
    "\n",
    "X = np.random.uniform(-2, 2, N)\n",
    "epsilon = np.random.normal(0, np.sqrt(2), N)\n",
    "\n",
    "y = 1 + 2*X + epsilon\n",
    "X_design = np.c_[np.ones(N), X]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8e17d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ols = np.linalg.inv(X_design.T @ X_design) @ X_design.T @ y\n",
    "y_pred_ols = X_design @ theta_ols\n",
    "\n",
    "ols_mse = mean_squared_error(y, y_pred_ols)\n",
    "ols_r2 = r2_score(y, y_pred_ols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d7424de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_closed_form(X, y, lam):\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    I = np.eye(n_features)\n",
    "    I[0,0] = 0 \n",
    "    \n",
    "    return np.linalg.inv(X.T @ X + lam*I) @ X.T @ y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8d4b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model  lambda  intercept    slope      MSE       R2\n",
      "0    OLS       0   1.137727 1.945275 1.949936 0.725824\n",
      "1  Ridge       1   1.137671 1.943850 1.949939 0.725823\n",
      "2  Ridge      10   1.137175 1.931119 1.950209 0.725785\n",
      "3  Ridge     100   1.132549 1.812414 1.974016 0.722438\n",
      "4  Ridge    1000   1.105658 1.122450 2.873516 0.595961\n",
      "5  Ridge   10000   1.071013 0.233509 5.947067 0.163796\n"
     ]
    }
   ],
   "source": [
    "lambdas = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"OLS\",\n",
    "    \"lambda\": 0,\n",
    "    \"intercept\": theta_ols[0],\n",
    "    \"slope\": theta_ols[1],\n",
    "    \"MSE\": ols_mse,\n",
    "    \"R2\": ols_r2\n",
    "})\n",
    "\n",
    "for lam in lambdas:\n",
    "    theta_ridge = ridge_closed_form(X_design, y, lam)\n",
    "    y_pred = X_design @ theta_ridge\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": \"Ridge\",\n",
    "        \"lambda\": lam,\n",
    "        \"intercept\": theta_ridge[0],\n",
    "        \"slope\": theta_ridge[1],\n",
    "        \"MSE\": mean_squared_error(y, y_pred),\n",
    "        \"R2\": r2_score(y, y_pred)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
